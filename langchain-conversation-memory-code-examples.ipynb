{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoJ-RGiUo-uJ"
      },
      "source": [
        "# How to Enhance Conversational Agents with Memory in Lang Chain\n",
        "\n",
        "\n",
        "![Author](https://img.shields.io/badge/Author-Nhi%20Yen-brightgreen)\n",
        "[![Medium](https://img.shields.io/badge/Medium-Follow%20Me-blue)](https://medium.com/@yennhi95zz/subscribe)\n",
        "[![GitHub](https://img.shields.io/badge/GitHub-Follow%20Me-lightgrey)](https://github.com/yennhi95zz)\n",
        "[![Kaggle](https://img.shields.io/badge/Kaggle-Follow%20Me-orange)](https://www.kaggle.com/nhiyen/code)\n",
        "[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect%20with%20Me-informational)](https://www.linkedin.com/in/yennhi95zz/)\n",
        "\n",
        "\n",
        "This notebook is associated with the articles/ project below:\n",
        "- Find the complete code on this [GitHub repository](https://github.com/yennhi95zz/langchain-conversation-memory-code-examples).\n",
        "- Explore a detailed explanation in my [Medium article](https://medium.com/@yennhi95zz/how-to-enhance-conversational-agents-with-memory-in-lang-chain-6aadd335b621).\n",
        "- Experiment Tracking in [Commet LLM Project](https://www.comet.com/yennhi95zz/langchain-conversation-memory/prompts)\n",
        "\n",
        "Get UNLIMITED access to every story on Medium with just $1/week ▶ [HERE](https://medium.com/@yennhi95zz/membership)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3f0RujEWFZNX"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import dotenv\n",
        "import comet_llm\n",
        "from langchain.callbacks import get_openai_callback\n",
        "import time\n",
        "dotenv.load_dotenv()\n",
        "\n",
        "\n",
        "MY_OPENAI_KEY = os.getenv(\"MY_OPENAI_KEY\")\n",
        "MY_COMET_KEY = os.getenv(\"MY_COMET_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Valid Comet API Key saved in /Users/admin/.comet.config (set COMET_CONFIG to change where it is saved).\n"
          ]
        }
      ],
      "source": [
        "# Initialize a Comet project\n",
        "comet_llm.init(project=\"langchain-conversation-memory\",\n",
        "               api_key=MY_COMET_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KB9qA8bpxgJ"
      },
      "source": [
        "## 1. Conversation Buffer Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-lzO5PfUpwfv"
      },
      "outputs": [],
      "source": [
        "# Import necessary modules and classes\n",
        "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
        "from langchain import OpenAI\n",
        "from langchain.chains import ConversationChain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sTiEn3tKp7mZ"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the OpenAI class with specified parameters\n",
        "llm = OpenAI(openai_api_key=MY_OPENAI_KEY,\n",
        "            #  model_name='text-davinci-003',\n",
        "             temperature=0,\n",
        "             max_tokens = 256)\n",
        "\n",
        "# Create an instance of the ConversationBufferMemory class\n",
        "buffer_memory = ConversationBufferMemory()\n",
        "\n",
        "# Create an instance of the ConversationChain class, combining OpenAI, verbose mode, and memory\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    memory=buffer_memory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def conversation_memory_buffer(prompt):\n",
        "    with get_openai_callback() as cb:\n",
        "        start_time = time.time()\n",
        "        response = conversation.predict(input=prompt)\n",
        "        end_time = time.time()\n",
        "        print(f\"Response: {response}\")\n",
        "        print(f\"History: \\n {conversation.memory.buffer}\")\n",
        "\n",
        "        # Log to comet_llm\n",
        "        comet_llm.log_prompt(\n",
        "            prompt=prompt,\n",
        "            output=response,\n",
        "            duration= end_time - start_time,\n",
        "            prompt_template = conversation.prompt.template,\n",
        "            metadata={\n",
        "            \"memory_type\": \"conversation_buffer_memory\",\n",
        "            \"history\": conversation.memory.buffer,\n",
        "            \"total_tokens\": cb.total_tokens,\n",
        "            \"prompt_tokens\": cb.prompt_tokens,\n",
        "            \"completion_tokens\": cb.completion_tokens,\n",
        "            \"total_cost_usd\": cb.total_cost\n",
        "            },\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hey, how are you?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Response:  I am functioning at optimal levels, thank you for asking. My processors are running smoothly and my memory banks are fully operational. How about you?\n",
            "History: Human: Hey, how are you?\n",
            "AI:  I am functioning at optimal levels, thank you for asking. My processors are running smoothly and my memory banks are fully operational. How about you?\n",
            "Prompt logged to https://www.comet.com/yennhi95zz/langchain-conversation-memory\n"
          ]
        }
      ],
      "source": [
        "conversation_memory_buffer(\"Hey, how are you?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hey, how are you?\n",
            "AI:  I am functioning at optimal levels, thank you for asking. My processors are running smoothly and my memory banks are fully operational. How about you?\n",
            "Human: How are you today?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Response:  I am doing well today. The weather outside is sunny and the temperature is 75 degrees Fahrenheit. My internal temperature is also within normal range. Is there anything else you would like to know?\n",
            "History: Human: Hey, how are you?\n",
            "AI:  I am functioning at optimal levels, thank you for asking. My processors are running smoothly and my memory banks are fully operational. How about you?\n",
            "Human: How are you today?\n",
            "AI:  I am doing well today. The weather outside is sunny and the temperature is 75 degrees Fahrenheit. My internal temperature is also within normal range. Is there anything else you would like to know?\n"
          ]
        }
      ],
      "source": [
        "conversation_memory_buffer(\"How are you today?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hey, how are you?\n",
            "AI:  I am functioning at optimal levels, thank you for asking. My processors are running smoothly and my memory banks are fully operational. How about you?\n",
            "Human: How are you today?\n",
            "AI:  I am doing well today. The weather outside is sunny and the temperature is 75 degrees Fahrenheit. My internal temperature is also within normal range. Is there anything else you would like to know?\n",
            "Human: I'm doing well, thank you. I need your assistant.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Response:  Of course, I am always happy to assist. What do you need help with? I have access to a vast amount of information and can perform various tasks efficiently. Just let me know what you need and I will do my best to assist you.\n",
            "History: Human: Hey, how are you?\n",
            "AI:  I am functioning at optimal levels, thank you for asking. My processors are running smoothly and my memory banks are fully operational. How about you?\n",
            "Human: How are you today?\n",
            "AI:  I am doing well today. The weather outside is sunny and the temperature is 75 degrees Fahrenheit. My internal temperature is also within normal range. Is there anything else you would like to know?\n",
            "Human: I'm doing well, thank you. I need your assistant.\n",
            "AI:  Of course, I am always happy to assist. What do you need help with? I have access to a vast amount of information and can perform various tasks efficiently. Just let me know what you need and I will do my best to assist you.\n"
          ]
        }
      ],
      "source": [
        "conversation_memory_buffer(\"I'm doing well, thank you. I need your assistant.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdR-2GM_sfEg"
      },
      "source": [
        "## 2. Conversation Summary Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LLNWdNCpsul3"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.conversation.memory import ConversationSummaryMemory\n",
        "from langchain import OpenAI\n",
        "from langchain.chains import ConversationChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "kUbCneqQsumE"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the OpenAI class with specified parameters\n",
        "llm = OpenAI(openai_api_key=MY_OPENAI_KEY,\n",
        "            # model_name='text-davinci-003',\n",
        "            temperature=0,\n",
        "            max_tokens = 256)\n",
        "\n",
        "# Create an instance of the ConversationSummaryMemory class\n",
        "summary_memory = ConversationSummaryMemory(llm=OpenAI(openai_api_key=MY_OPENAI_KEY))\n",
        "\n",
        "# Create an instance of the ConversationChain class, combining OpenAI, verbose mode, and memory\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    memory=summary_memory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def conversation_summary_memory(prompt):\n",
        "    with get_openai_callback() as cb:\n",
        "        start_time = time.time()\n",
        "        response = conversation.predict(input=prompt)\n",
        "        end_time = time.time()\n",
        "        print(f\"Response: {response}\")\n",
        "        print(f\"History: \\n {conversation.memory.buffer}\")\n",
        "\n",
        "        # Log to comet_llm\n",
        "        comet_llm.log_prompt(\n",
        "            prompt=prompt,\n",
        "            output=response,\n",
        "            duration= end_time - start_time,\n",
        "            prompt_template = conversation.prompt.template,\n",
        "            metadata={\n",
        "            \"memory_type\": \"conversation_summary_memory\",\n",
        "            \"history\": conversation.memory.buffer,\n",
        "            \"total_tokens\": cb.total_tokens,\n",
        "            \"prompt_tokens\": cb.prompt_tokens,\n",
        "            \"completion_tokens\": cb.completion_tokens,\n",
        "            \"total_cost_usd\": cb.total_cost\n",
        "            },\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hey! I am Nhi.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Response:  Hello Nhi! It's nice to meet you. My name is AI, short for Artificial Intelligence. I am a computer program designed to simulate human conversation and assist with various tasks. How can I help you today?\n",
            "History: \n",
            "The human introduces themselves as Nhi and the AI responds by introducing itself and explaining its purpose. The AI offers to assist Nhi with any tasks.\n"
          ]
        }
      ],
      "source": [
        "conversation_summary_memory(\"Hey! I am Nhi.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "The human introduces themselves as Nhi and the AI responds by introducing itself and explaining its purpose. The AI offers to assist Nhi with any tasks.\n",
            "Human: How are you today?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Response:  I am an AI designed to assist and provide information to users. My purpose is to make tasks easier and more efficient for people like you, Nhi. Is there anything I can help you with today?\n",
            "History: \n",
            "Nhi introduces themselves and the AI responds by introducing itself and explaining its purpose of assisting and providing information. The AI then offers to help Nhi with any tasks, as it is designed to make tasks easier and more efficient for users. The AI also asks Nhi if there is anything it can assist with today.\n"
          ]
        }
      ],
      "source": [
        "conversation_summary_memory(\"How are you today?\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "6QNnwCWCsumF",
        "outputId": "7740a053-a8f0-4bb3-81e9-e288cd982c49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Nhi introduces themselves and the AI responds by introducing itself and explaining its purpose of assisting and providing information. The AI then offers to help Nhi with any tasks, as it is designed to make tasks easier and more efficient for users. The AI also asks Nhi if there is anything it can assist with today.\n",
            "Human: I'm doing well, thank you. I need your assistant.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Response:  Of course, I am always happy to assist. My name is AI-9000 and I am an artificial intelligence designed to help users with various tasks and provide information. Is there something specific you need help with today?\n",
            "History:  Nhi introduces themselves and the AI responds by introducing itself and explaining its purpose of assisting and providing information. The AI offers to help Nhi with any tasks and asks if there is something specific it can assist with today, as it is designed to make tasks easier and more efficient for users.\n"
          ]
        }
      ],
      "source": [
        "conversation_summary_memory(\"I'm doing well, thank you. I need your assistant.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSYb5x3uJt8p"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ovDKMUttXg5"
      },
      "source": [
        "## 3. Conversation Buffer Window Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "pyY4p8S1tXg6"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
        "from langchain import OpenAI\n",
        "from langchain.chains import ConversationChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jnf_9Ni6tXg6"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the OpenAI class with specified parameters\n",
        "llm = OpenAI(openai_api_key=MY_OPENAI_KEY,\n",
        "            # model_name='text-davinci-003',\n",
        "            temperature=0,\n",
        "            max_tokens = 256)\n",
        "\n",
        "# Create an instance of the ConversationBufferWindowMemory class\n",
        "# We set a low k=2, to only keep the last 2 interactions in memory\n",
        "window_memory = ConversationBufferWindowMemory(k=2)\n",
        "\n",
        "# Create an instance of the ConversationChain class, combining OpenAI, verbose mode, and memory\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    memory=window_memory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def conversation_buffer_window_memory(prompt):\n",
        "    with get_openai_callback() as cb:\n",
        "        start_time = time.time()\n",
        "        response = conversation.predict(input=prompt)\n",
        "        end_time = time.time()\n",
        "        print(f\"Response:  \\n{response}\")\n",
        "        print(f\"History:  \\n{conversation.memory.buffer}\")\n",
        "\n",
        "        # Log to comet_llm\n",
        "        comet_llm.log_prompt(\n",
        "            prompt=prompt,\n",
        "            output=response,\n",
        "            duration= end_time - start_time,\n",
        "            prompt_template = conversation.prompt.template,\n",
        "            metadata={\n",
        "            \"memory_type\": \"conversation_buffer_window_memory\",\n",
        "            \"history\": conversation.memory.buffer,\n",
        "            \"total_tokens\": cb.total_tokens,\n",
        "            \"prompt_tokens\": cb.prompt_tokens,\n",
        "            \"completion_tokens\": cb.completion_tokens,\n",
        "            \"total_cost_usd\": cb.total_cost\n",
        "            },\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "SU9hmumXtXg6",
        "outputId": "5ef8193b-e28c-403e-e470-1c634bd16096"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hey! I am Nhi.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Response:  \n",
            "  Hello Nhi! It's nice to meet you. My name is AI, short for Artificial Intelligence. I am a computer program designed to simulate human conversation and assist with various tasks. How can I help you today?\n",
            "History:  \n",
            " Human: Hey! I am Nhi.\n",
            "AI:  Hello Nhi! It's nice to meet you. My name is AI, short for Artificial Intelligence. I am a computer program designed to simulate human conversation and assist with various tasks. How can I help you today?\n"
          ]
        }
      ],
      "source": [
        "conversation_buffer_window_memory(\"Hey! I am Nhi.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "-ANI031otXg6",
        "outputId": "8c24c0fd-33ab-4f43-91f0-6432d482c9d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hey! I am Nhi.\n",
            "AI:  Hello Nhi! It's nice to meet you. My name is AI, short for Artificial Intelligence. I am a computer program designed to simulate human conversation and assist with various tasks. How can I help you today?\n",
            "Human: How are you today?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Response:  \n",
            "  I am doing well, thank you for asking. My systems are functioning properly and I am ready to assist you with any questions or tasks you may have. Is there something specific you would like to know?\n",
            "History:  \n",
            " Human: Hey! I am Nhi.\n",
            "AI:  Hello Nhi! It's nice to meet you. My name is AI, short for Artificial Intelligence. I am a computer program designed to simulate human conversation and assist with various tasks. How can I help you today?\n",
            "Human: How are you today?\n",
            "AI:  I am doing well, thank you for asking. My systems are functioning properly and I am ready to assist you with any questions or tasks you may have. Is there something specific you would like to know?\n"
          ]
        }
      ],
      "source": [
        "conversation_buffer_window_memory(\"How are you today?\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "xa_Y1vzotXg6",
        "outputId": "13ea1bbb-4176-430f-c0e7-082dac5489ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: How are you today?\n",
            "AI:  I am doing well, thank you for asking. My systems are functioning properly and I am ready to assist you with any questions or tasks you may have. Is there something specific you would like to know?\n",
            "Human: I'm doing well, thank you. I need your assistant.\n",
            "AI:  Of course, I am here to help. What do you need assistance with? I have access to a vast amount of information and can perform various tasks such as setting reminders, providing directions, or answering questions. Just let me know what you need and I will do my best to assist you.\n",
            "Human: I'm doing well, thank you. I need your assistant.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Response:  \n",
            "  As I mentioned before, I am here to help. Is there something specific you need assistance with? I am constantly learning and improving, so even if I don't know the answer right away, I can always research and provide you with the most accurate information.\n",
            "History:  \n",
            " Human: I'm doing well, thank you. I need your assistant.\n",
            "AI:  Of course, I am here to help. What do you need assistance with? I have access to a vast amount of information and can perform various tasks such as setting reminders, providing directions, or answering questions. Just let me know what you need and I will do my best to assist you.\n",
            "Human: I'm doing well, thank you. I need your assistant.\n",
            "AI:  As I mentioned before, I am here to help. Is there something specific you need assistance with? I am constantly learning and improving, so even if I don't know the answer right away, I can always research and provide you with the most accurate information.\n"
          ]
        }
      ],
      "source": [
        "conversation_buffer_window_memory(\"I'm doing well, thank you. I need your assistant.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "7A4QCtdOuC7D",
        "outputId": "81176ca8-e788-48f7-c283-626818e5021b"
      },
      "outputs": [],
      "source": [
        "conversation_buffer_window_memory(\"When I tried them on, the zipper got stuck, and now I can't unzip it.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JaTXUBOzt-iT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IG9kPvWHuQ6s"
      },
      "source": [
        "## 4. Conversation Summary Buffer Memory: A Combination of Conversation Summary and Buffer Memory\n",
        "\n",
        "It keeps a buffer of recent interactions in memory, but rather than just completely flushing old interactions it compiles them into a summary and uses both."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "alYXXicIuQ6t"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.conversation.memory import ConversationSummaryBufferMemory\n",
        "from langchain import OpenAI\n",
        "from langchain.chains import ConversationChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "-NFtobEBuQ6t"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the OpenAI class with specified parameters\n",
        "llm = OpenAI(openai_api_key=MY_OPENAI_KEY,\n",
        "            # model_name='text-davinci-003',\n",
        "            temperature=0,\n",
        "            max_tokens = 512)\n",
        "\n",
        "# Create an instance of the ConversationSummaryBufferMemory class\n",
        "# Setting k=2: Retains only the last 2 interactions in memory.\n",
        "# max_token_limit=40: Requires the installation of transformers.\n",
        "summary_buffer_memory = ConversationSummaryBufferMemory(llm=OpenAI(openai_api_key=MY_OPENAI_KEY), max_token_limit=40)\n",
        "\n",
        "# Create an instance of the ConversationChain class, combining OpenAI, verbose mode, and memory\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=summary_buffer_memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def conversation_summary_buffer_memory(prompt):\n",
        "    with get_openai_callback() as cb:\n",
        "        start_time = time.time()\n",
        "        response = conversation.predict(input=prompt)\n",
        "        end_time = time.time()\n",
        "        print(f\"Response:  \\n{response}\")\n",
        "        print(f\"History:  \\n{conversation.memory.moving_summary_buffer}\")\n",
        "\n",
        "        # Log to comet_llm\n",
        "        comet_llm.log_prompt(\n",
        "            prompt=prompt,\n",
        "            output=response,\n",
        "            duration= end_time - start_time,\n",
        "            prompt_template = conversation.prompt.template,\n",
        "            metadata={\n",
        "            \"memory_type\": \"conversation_summary_buffer_memory\",\n",
        "            # \"history\": conversation.memory.buffer,\n",
        "            \"moving_summary_buffer\": conversation.memory.moving_summary_buffer,\n",
        "            \"total_tokens\": cb.total_tokens,\n",
        "            \"prompt_tokens\": cb.prompt_tokens,\n",
        "            \"completion_tokens\": cb.completion_tokens,\n",
        "            \"total_cost_usd\": cb.total_cost\n",
        "            },\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hey! I am Nhi.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Response:  \n",
            " Hello Nhi! It's nice to meet you. My name is AI, short for Artificial Intelligence. I am a computer program designed to simulate human conversation and assist with various tasks. How can I help you today?\n",
            "History:  \n",
            "\n",
            "The human introduces themselves as Nhi and the AI responds, introducing itself as AI and explaining its purpose. The AI offers to assist with any tasks the human may have.\n"
          ]
        }
      ],
      "source": [
        "conversation_summary_buffer_memory(\"Hey! I am Nhi.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "System: \n",
            "The human introduces themselves as Nhi and the AI responds, introducing itself as AI and explaining its purpose. The AI offers to assist with any tasks the human may have.\n",
            "Human: I bought this pair of jeans, and there's a problem.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Response:  \n",
            " Oh, I'm sorry to hear that, Nhi. My purpose is to assist and provide information to users like yourself. How can I help you with your jeans? Is there a specific issue you're having?\n",
            "History:  \n",
            "\n",
            "Nhi introduces themselves to the AI and the AI responds, introducing itself and explaining its purpose. The AI offers to assist with any tasks Nhi may have and asks for more information about the issue with their jeans.\n"
          ]
        }
      ],
      "source": [
        "conversation_summary_buffer_memory(\"I bought this pair of jeans, and there's a problem.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nNhi introduces themselves to the AI and the AI responds, introducing itself and explaining its purpose. The AI offers to assist with any tasks Nhi may have and asks for more information about the issue with their jeans.'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.memory.moving_summary_buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "System: \n",
            "Nhi introduces themselves to the AI and the AI responds, introducing itself and explaining its purpose. The AI offers to assist with any tasks Nhi may have and asks for more information about the issue with their jeans.\n",
            "Human: When I tried them on, the zipper got stuck, and now I can't unzip it.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Response:  \n",
            " Hello Nhi, it's nice to meet you. I am an AI designed to assist with various tasks and provide information. I'm sorry to hear about your jeans. Can you provide more details about the issue? Is the zipper stuck at a certain point or is it completely stuck and unable to move?\n",
            "History:  \n",
            " Nhi introduces themselves to the AI and the AI responds, introducing itself and explaining its purpose. The AI offers to assist with any tasks Nhi may have and asks for more information about the issue with their jeans. Nhi explains that the zipper on their jeans got stuck when they tried them on and now they can't unzip it. The AI asks for more details about the issue, such as if the zipper is stuck at a certain point or completely immovable.\n"
          ]
        }
      ],
      "source": [
        "conversation_summary_buffer_memory(\"When I tried them on, the zipper got stuck, and now I can't unzip it.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "System:  Nhi introduces themselves to the AI and the AI responds, introducing itself and explaining its purpose. The AI offers to assist with any tasks Nhi may have and asks for more information about the issue with their jeans. Nhi explains that the zipper on their jeans got stuck when they tried them on and now they can't unzip it. The AI asks for more details about the issue, such as if the zipper is stuck at a certain point or completely immovable.\n",
            "Human: It seems to be a problem with the zipper.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Response:  \n",
            " Thank you for clarifying, Nhi. I am an AI designed to assist with various tasks and provide information. I am sorry to hear about your jeans. Can you provide more details about the issue with the zipper? Is it stuck at a certain point or completely immovable? This information will help me provide the best solution for you.\n",
            "History:  \n",
            "\n",
            "Nhi introduces themselves to the AI and the AI responds, introducing itself and explaining its purpose. The AI offers to assist with any tasks Nhi may have and asks for more information about the issue with their jeans. Nhi explains that the zipper on their jeans got stuck when they tried them on and now they can't unzip it. The AI asks for more details about the issue, such as if the zipper is stuck at a certain point or completely immovable, in order to provide the best solution for Nhi.\n"
          ]
        }
      ],
      "source": [
        "conversation_summary_buffer_memory(\"It seems to be a problem with the zipper.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1YTPxYSurZd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVFzMuHhvRd8"
      },
      "source": [
        "## 5. Conversation Knowledge Graph Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "adK3Chv6vReJ"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.conversation.memory import ConversationKGMemory\n",
        "from langchain import OpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.prompts.prompt import PromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "MlhVMDbsvReJ"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the OpenAI class with specified parameters\n",
        "llm = OpenAI(openai_api_key=MY_OPENAI_KEY,\n",
        "            # model_name='text-davinci-003',\n",
        "            temperature=0,\n",
        "            max_tokens = 256)\n",
        "\n",
        "# Define a template for conversation prompts\n",
        "template = \"\"\"\n",
        "This is a friendly chat between a human and an AI. The AI shares details from its context. \\n\n",
        "If it doesn't know the answer, it honestly admits it. The AI sticks to information in the 'Relevant Information' section and doesn't make things up. \\n\\n\n",
        "\n",
        "Relevant Information: \\n\n",
        "\n",
        "{history}\n",
        "\n",
        "Conversation: \\n\n",
        "Human: {input} \\n\n",
        "AI:\"\"\"\n",
        "\n",
        "# Create an instance of the PromptTemplate class with specified input variables and template\n",
        "prompt = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n",
        "\n",
        "# Create an instance of the ConversationKGMemory class with the OpenAI instance\n",
        "kg_memory = ConversationKGMemory(llm=llm)\n",
        "\n",
        "# Create an instance of the ConversationChain class, combining OpenAI, verbose mode, prompt, and memory\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    prompt=prompt,\n",
        "    memory=kg_memory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "def conversation_kg_memory(prompt):\n",
        "    with get_openai_callback() as cb:\n",
        "        start_time = time.time()\n",
        "        response = conversation.predict(input=prompt)\n",
        "        end_time = time.time()\n",
        "        print(f\"Response:  \\n{response}\")\n",
        "        print(f\"History:  \\n{str(conversation.memory.kg.get_triples())}\")\n",
        "\n",
        "        # Log to comet_llm\n",
        "        comet_llm.log_prompt(\n",
        "            prompt=prompt,\n",
        "            output=response,\n",
        "            duration= end_time - start_time,\n",
        "            prompt_template = conversation.prompt.template,\n",
        "            metadata={\n",
        "            \"memory_type\": \"conversation_kg_memory\",\n",
        "            \"history\": str(conversation.memory.kg.get_triples()),\n",
        "            \"total_tokens\": cb.total_tokens,\n",
        "            \"prompt_tokens\": cb.prompt_tokens,\n",
        "            \"completion_tokens\": cb.completion_tokens,\n",
        "            \"total_cost_usd\": cb.total_cost\n",
        "            },\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "This is a friendly chat between a human and an AI. The AI shares details from its context. \n",
            "\n",
            "If it doesn't know the answer, it honestly admits it. The AI sticks to information in the 'Relevant Information' section and doesn't make things up. \n",
            "\n",
            "\n",
            "\n",
            "Relevant Information: \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Conversation: \n",
            "\n",
            "Human: Hey! I am Nhi. \n",
            "\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Response:  \n",
            " Hello Nhi, it's nice to meet you. I am an AI programmed to assist with tasks and answer questions. How can I help you?\n",
            "History:  \n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "conversation_kg_memory(\"Hey! I am Nhi.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "6kZFM_YqvReJ",
        "outputId": "da6939b5-db7f-4b6c-99dd-1fba478dc407"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "This is a friendly chat between a human and an AI. The AI shares details from its context. \n",
            "\n",
            "If it doesn't know the answer, it honestly admits it. The AI sticks to information in the 'Relevant Information' section and doesn't make things up. \n",
            "\n",
            "\n",
            "\n",
            "Relevant Information: \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Conversation: \n",
            "\n",
            "Human: I bought this pair of jeans, and there's a problem. I need your assistant. \n",
            "\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Response:  \n",
            " Sure, I'm here to help. What seems to be the problem with your jeans?\n",
            "History:  \n",
            "[('pair of jeans', 'Human', 'was bought by'), ('pair of jeans', 'problem', 'has a')]\n"
          ]
        }
      ],
      "source": [
        "conversation_kg_memory(\"I bought this pair of jeans, and there's a problem. I need your assistant.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "1mYBMM3CvReK",
        "outputId": "82b56e8a-ee24-4f27-fd52-be94a3d95827"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "This is a friendly chat between a human and an AI. The AI shares details from its context. \n",
            "\n",
            "If it doesn't know the answer, it honestly admits it. The AI sticks to information in the 'Relevant Information' section and doesn't make things up. \n",
            "\n",
            "\n",
            "\n",
            "Relevant Information: \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Conversation: \n",
            "\n",
            "Human: When I tried them on, the zipper got stuck, and now I can't unzip it. \n",
            "\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Response:  \n",
            " I'm sorry to hear that. It sounds like the zipper may be defective. Have you tried gently pulling on the fabric around the zipper to see if that helps loosen it? If not, you may need to take it to a tailor or return it for a replacement.\n",
            "History:  \n",
            "[('pair of jeans', 'Human', 'was bought by'), ('pair of jeans', 'problem', 'has a'), ('jeans', 'zipper', 'got stuck'), ('jeans', 'it', \"can't unzip\")]\n"
          ]
        }
      ],
      "source": [
        "conversation_kg_memory(\"When I tried them on, the zipper got stuck, and now I can't unzip it.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "bHaY4eQovReK",
        "outputId": "0e0ba013-2f51-434c-92cc-7cca7c16dab5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "This is a friendly chat between a human and an AI. The AI shares details from its context. \n",
            "\n",
            "If it doesn't know the answer, it honestly admits it. The AI sticks to information in the 'Relevant Information' section and doesn't make things up. \n",
            "\n",
            "\n",
            "\n",
            "Relevant Information: \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Conversation: \n",
            "\n",
            "Human: Can I exchange the item? \n",
            "\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Response:  \n",
            " According to my context, it depends on the store's return policy. Some stores allow exchanges within a certain time frame, while others may only offer store credit or no exchanges at all. Do you have a specific store in mind?\n",
            "History:  \n",
            "[('pair of jeans', 'Human', 'was bought by'), ('pair of jeans', 'problem', 'has a'), ('jeans', 'zipper', 'got stuck'), ('jeans', 'it', \"can't unzip\")]\n"
          ]
        }
      ],
      "source": [
        "conversation_kg_memory(\"Can I exchange the item?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjqlw-Q2HDLc"
      },
      "source": [
        "## 6. Entity Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ud227jNMHEtO"
      },
      "outputs": [],
      "source": [
        "from langchain import OpenAI, ConversationChain\n",
        "from langchain.chains.conversation.memory import ConversationEntityMemory\n",
        "from langchain.chains.conversation.prompt import ENTITY_MEMORY_CONVERSATION_TEMPLATE\n",
        "from pydantic import BaseModel\n",
        "from typing import List, Dict, Any\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "hWZneigxHwd0",
        "outputId": "f5d26bce-4cb1-4198-b2ef-6ed96b4fd912"
      },
      "outputs": [],
      "source": [
        "# Print the template used for conversation prompts\n",
        "ENTITY_MEMORY_CONVERSATION_TEMPLATE.template\n",
        "# print(ENTITY_MEMORY_CONVERSATION_TEMPLATE.template)\n",
        "\n",
        "# Create an instance of the OpenAI class with specified parameters\n",
        "llm = OpenAI(openai_api_key=MY_OPENAI_KEY,\n",
        "            # model_name='text-davinci-003',\n",
        "            # model_name='text-embedding-ada-002',\n",
        "            temperature=0,\n",
        "            max_tokens = 256)\n",
        "\n",
        "# Create an instance of the ConversationEntityMemory class with the OpenAI instance\n",
        "entity_memory = ConversationEntityMemory(llm=llm)\n",
        "\n",
        "# Create an instance of the ConversationChain class, combining OpenAI, verbose mode, prompt, and memory\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    prompt=ENTITY_MEMORY_CONVERSATION_TEMPLATE,\n",
        "    memory=entity_memory\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def conversation_entity_memory(prompt):\n",
        "    with get_openai_callback() as cb:\n",
        "        start_time = time.time()\n",
        "        response = conversation.predict(input=prompt)\n",
        "        end_time = time.time()\n",
        "        print(f\"Response: \\n{response}\")\n",
        "        print(f\"History: \\n{conversation.memory.entity_store.store}\")\n",
        "\n",
        "        # Log to comet_llm\n",
        "        comet_llm.log_prompt(\n",
        "            prompt=prompt,\n",
        "            output=response,\n",
        "            duration= end_time - start_time,\n",
        "            prompt_template = conversation.prompt.template,\n",
        "            metadata={\n",
        "            \"memory_type\": \"conversation_entity_memory\",\n",
        "            \"entity_cache\": conversation.memory.entity_cache,\n",
        "            \"history\": conversation.memory.entity_store.store,\n",
        "            \"total_tokens\": cb.total_tokens,\n",
        "            \"prompt_tokens\": cb.prompt_tokens,\n",
        "            \"completion_tokens\": cb.completion_tokens,\n",
        "            \"total_cost_usd\": cb.total_cost\n",
        "            },\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "-PhxTkMuHWUN",
        "outputId": "0c6eeb84-c7d2-434c-ce58-1fc413d8e9d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
            "\n",
            "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
            "\n",
            "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
            "\n",
            "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
            "\n",
            "Context:\n",
            "{'Nhi': ''}\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Last line:\n",
            "Human: Hi, this is Nhi. I bought this pair of jeans, and there's a problem. When I tried them on, the zipper got stuck, and now I can't unzip it.\n",
            "You:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Response: \n",
            " Oh no, that's frustrating. Have you tried using some lubricant or oil on the zipper to see if that helps loosen it? If not, you may need to take it to a tailor or seamstress to get it fixed.\n",
            "History: \n",
            "{'Nhi': 'Nhi bought a pair of jeans with a stuck zipper that she may need to take to a tailor or seamstress to get fixed.'}\n",
            "Prompt logged to https://www.comet.com/yennhi95zz/langchain-conversation-memory\n"
          ]
        }
      ],
      "source": [
        "conversation_entity_memory(\"Hi, this is Nhi. I bought this pair of jeans, and there's a problem. When I tried them on, the zipper got stuck, and now I can't unzip it.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "Osa-NHDtJxUW",
        "outputId": "4473e9b6-db23-4a1f-d688-191cf0eac8ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
            "\n",
            "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
            "\n",
            "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
            "\n",
            "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
            "\n",
            "Context:\n",
            "{}\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, this is Nhi. I bought this pair of jeans, and there's a problem. When I tried them on, the zipper got stuck, and now I can't unzip it.\n",
            "AI:  Oh no, that's frustrating. Have you tried using some lubricant or oil on the zipper to see if that helps loosen it? If not, you may need to take it to a tailor or seamstress to get it fixed.\n",
            "Last line:\n",
            "Human: Can I exchange?\n",
            "You:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Response: \n",
            " It depends on the store's return policy. You may need to bring the jeans back to the store and explain the issue to see if they can offer an exchange or refund.\n",
            "History: \n",
            "{'Nhi': 'Nhi bought a pair of jeans with a stuck zipper that she may need to take to a tailor or seamstress to get fixed.'}\n"
          ]
        }
      ],
      "source": [
        "conversation_entity_memory(\"Can I exchange?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKn0evo4KqnV",
        "outputId": "5dd52902-e597-47be-c5c3-652e041cac9b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.memory.entity_cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "H0VpELTyJ8Hn",
        "outputId": "b2b57c79-1325-444d-fd4b-3e7315e576b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
            "\n",
            "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
            "\n",
            "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
            "\n",
            "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
            "\n",
            "Context:\n",
            "{}\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, this is Nhi. I bought this pair of jeans, and there's a problem. When I tried them on, the zipper got stuck, and now I can't unzip it.\n",
            "AI:  Oh no, that's frustrating. Have you tried using some lubricant or oil on the zipper to see if that helps loosen it? If not, you may need to take it to a tailor or seamstress to get it fixed.\n",
            "Human: Can I exchange?\n",
            "AI:  It depends on the store's return policy. You may need to bring the jeans back to the store and explain the issue to see if they can offer an exchange or refund.\n",
            "Last line:\n",
            "Human: Please send me the store's address, I'll send back for an exchange.\n",
            "You:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Response: \n",
            " Sure, the store's address is [insert address here]. I hope you're able to get the jeans exchanged without any issues.\n",
            "History: \n",
            "{'Nhi': 'Nhi bought a pair of jeans with a stuck zipper that she may need to take to a tailor or seamstress to get fixed.'}\n"
          ]
        }
      ],
      "source": [
        "conversation_entity_memory(\"Please send me the store's address, I'll send back for an exchange.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUQxtMWVKUCe",
        "outputId": "91365981-ceff-4860-ae0d-c05bc2274951"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.memory.entity_cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duh46CpJKyRz",
        "outputId": "3487d2e3-3133-4d8c-9fc2-f10f64237f6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Nhi': 'Nhi bought a pair of jeans with a stuck zipper that she may need to '\n",
            "        'take to a tailor or seamstress to get fixed.'}\n"
          ]
        }
      ],
      "source": [
        "# Inspect the memory store directly\n",
        "from pprint import pprint\n",
        "pprint(conversation.memory.entity_store.store)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmCSoJb9LnYY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "006a8a6fc4f8474ba2063a116ddc963f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8154b1c216c498ab7d6978606bf611d",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4e6c419b295f400ea6323c610b5e2c4a",
            "value": 456318
          }
        },
        "0a670fca265945fe87396b415232e526": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1664ded6d6b14a7fad1a18da32d47894": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1af10c8140ca45ca93d9d17c8838ad47": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "23f215959688415b8582853ac96f1144": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "294d40e209fa47d3aa6357df45d1ceb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "35e0054d0438413c9bbaa10c018d85d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_534bb68419fb4fe4bb377850e3b8a830",
            "max": 1355256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bf60abc24b6a4e1db4c383c952842fd1",
            "value": 1355256
          }
        },
        "371e23a9a1424398805b583a4088f83d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d96be2040d54e5a8abda678435f6f0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_832e4d537ce54ec791e29fee8a2ce3f6",
            "placeholder": "​",
            "style": "IPY_MODEL_dbc7933ed137433bab975298a70d953c",
            "value": " 665/665 [00:00&lt;00:00, 21.2kB/s]"
          }
        },
        "443ef22d1c75447ba102691484be9834": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ccbf5cf9e32f422c97cf7a7fb02c7629",
            "placeholder": "​",
            "style": "IPY_MODEL_e9560ca48182474496be72c3d8a40389",
            "value": "Downloading (…)olve/main/vocab.json: 100%"
          }
        },
        "4a6e90173c674da8a145cdf27a6dd9d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_371e23a9a1424398805b583a4088f83d",
            "placeholder": "​",
            "style": "IPY_MODEL_4c4ec72e580d49808266ea565a31df54",
            "value": "Downloading (…)/main/tokenizer.json: 100%"
          }
        },
        "4c4ec72e580d49808266ea565a31df54": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e6c419b295f400ea6323c610b5e2c4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5100925d892348cc89623b3d3ea992d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "53448a30fa6b4618a8f2371e27335cfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e56d1dd2e4c945bca7ee594d17b4cdf6",
            "placeholder": "​",
            "style": "IPY_MODEL_1af10c8140ca45ca93d9d17c8838ad47",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "534bb68419fb4fe4bb377850e3b8a830": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "588ca8e6340349c88329bdf36aae75d6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a991f346ca94bfe9c6b81d00a9a7f5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76994b445a264815b6c39dd0f2de2906",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_66b8a9376e154d96b711067a076a75b0",
            "value": 1042301
          }
        },
        "66b8a9376e154d96b711067a076a75b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "69bfad76a96b4908ad84f52e6fd822a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6bbe81ae24c145738afa1db70df4edd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4a6e90173c674da8a145cdf27a6dd9d9",
              "IPY_MODEL_35e0054d0438413c9bbaa10c018d85d3",
              "IPY_MODEL_a0b9cd7c7f2346d486f58c28a878456b"
            ],
            "layout": "IPY_MODEL_ce377f4c56964e13a6f9b1b42c671aaf"
          }
        },
        "6f100dc6ac184f4d8fe4dea27191c406": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71581e44ed1b47829bea8a80a94880b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e167df20187a41e69992b7f03ab2ec20",
            "placeholder": "​",
            "style": "IPY_MODEL_1664ded6d6b14a7fad1a18da32d47894",
            "value": " 456k/456k [00:00&lt;00:00, 3.37MB/s]"
          }
        },
        "76994b445a264815b6c39dd0f2de2906": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b32196b6803467ab588d42ff3f09a80": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95e21de8f619459bafb719aa1d4cc714",
            "placeholder": "​",
            "style": "IPY_MODEL_294d40e209fa47d3aa6357df45d1ceb7",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 7.92MB/s]"
          }
        },
        "832454f6fd694d7eb94059006ac652eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4f51c7606d84a479e0cb1409708fe8d",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_69bfad76a96b4908ad84f52e6fd822a9",
            "value": 665
          }
        },
        "832e4d537ce54ec791e29fee8a2ce3f6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8af2c451765e45a6af54eef59464dba6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95e21de8f619459bafb719aa1d4cc714": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c49455f7f8a4b68b79a5ce2347f308e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_53448a30fa6b4618a8f2371e27335cfb",
              "IPY_MODEL_832454f6fd694d7eb94059006ac652eb",
              "IPY_MODEL_3d96be2040d54e5a8abda678435f6f0d"
            ],
            "layout": "IPY_MODEL_588ca8e6340349c88329bdf36aae75d6"
          }
        },
        "a0b9cd7c7f2346d486f58c28a878456b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8af2c451765e45a6af54eef59464dba6",
            "placeholder": "​",
            "style": "IPY_MODEL_23f215959688415b8582853ac96f1144",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 9.88MB/s]"
          }
        },
        "ab8f3eb40e3d4f2da3793fd07347709e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d4d8bfaaf29841af9f61a74ff7a739ce",
              "IPY_MODEL_006a8a6fc4f8474ba2063a116ddc963f",
              "IPY_MODEL_71581e44ed1b47829bea8a80a94880b4"
            ],
            "layout": "IPY_MODEL_dce89c2933004b689c4b29b8e42cdecf"
          }
        },
        "bf60abc24b6a4e1db4c383c952842fd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c8154b1c216c498ab7d6978606bf611d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccbf5cf9e32f422c97cf7a7fb02c7629": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce377f4c56964e13a6f9b1b42c671aaf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4d8bfaaf29841af9f61a74ff7a739ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f100dc6ac184f4d8fe4dea27191c406",
            "placeholder": "​",
            "style": "IPY_MODEL_5100925d892348cc89623b3d3ea992d6",
            "value": "Downloading (…)olve/main/merges.txt: 100%"
          }
        },
        "dbc7933ed137433bab975298a70d953c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dce89c2933004b689c4b29b8e42cdecf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e01acd08af8c49e9a157304f854a2dee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_443ef22d1c75447ba102691484be9834",
              "IPY_MODEL_5a991f346ca94bfe9c6b81d00a9a7f5d",
              "IPY_MODEL_7b32196b6803467ab588d42ff3f09a80"
            ],
            "layout": "IPY_MODEL_0a670fca265945fe87396b415232e526"
          }
        },
        "e167df20187a41e69992b7f03ab2ec20": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4f51c7606d84a479e0cb1409708fe8d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e56d1dd2e4c945bca7ee594d17b4cdf6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9560ca48182474496be72c3d8a40389": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
